[
["index.html", "Dashboards project Chapter 1 Introduction 1.1 Executive summary 1.2 What is an automated analysis? 1.3 Why not have one project for each automated analysis? 1.4 Important repositories", " Dashboards project Richard White 2018-10-18 Chapter 1 Introduction 1.1 Executive summary The dashboards project is a project at FHI concerned with running automated analyses on data. In principle, the dashboards project is split up into three parts: The overarching infrastructure (i.e. Docker containers, continuous integration, chron jobs, etc.) The R package for each automated analysis Integrating the R package into the physical system (e.g. specifying where the data is) 1.2 What is an automated analysis? An automated analysis is any analysis that: Will be repeated multiple times in the future Always has an input dataset with consistent file structure Always has the same expected output (e.g. tables, graphs, reports) 1.3 Why not have one project for each automated analysis? Automated analyses have a lot of code and infrastructure in common. Automated analyses: Need their code to be tested via unit testing to ensure the results are correct Need their code to be tested via integration testing to ensure everything runs Need to be run at certain times Need to be able to send emails notifying people that the analyses have finished running Need to make their results accessible to the relevant people By combining them all in one umbrella project we can force everyone to use the same infrastructure and coding principles, so we: Only need to solve a problem once Only need to maintain one system Can easily work on multiple projects, as we all speak the same language 1.4 Important repositories 1.4.1 Infrastructure https://github.com/raubreywhite/dashboards_control/ (private) This contains the Docker files, cronfiles, all bash scripts, etc. https://folkehelseinstituttet.github.io/dashboards/ (this one) This contains the R executable for each automated analysis. https://folkehelseinstituttet.github.io/fhi/ This is an R package that contains helper functions. 1.4.2 Automated analyses https://folkehelseinstituttet.github.io/dashboards_sykdomspuls/ https://folkehelseinstituttet.github.io/dashboards_normomo/ https://folkehelseinstituttet.github.io/dashboards_sykdomspuls_pdf/ https://folkehelseinstituttet.github.io/dashboards_sykdomspuls_log/ "],
["integrating-the-r-package-into-the-physical-system.html", "Chapter 2 Integrating the R package into the physical system 2.1 Summary 2.2 RunProcess.R 2.3 0_run.sh 2.4 RunTest.R", " Chapter 2 Integrating the R package into the physical system 2.1 Summary An R package is not enough to run an analysis – something needs to physically call the functions inside the R package. That is, the R package needs to be integrated into the physical system. Everything related to integrating the R package into the physical system lives in the dashboards repository. Inside the dashboards repository we have: - dev/ |-- src/ |-- sykdomspuls/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R |-- normomo/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R |-- sykdomspuls_log/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R |-- sykdomspuls_pdf/ |-- 0_run.sh |-- RunProcess.R |-- RunTest.R 2.2 RunProcess.R 2.2.1 Purpose An automated analysis needs to: Know the location of the data/results folders. Check for new data in these folders. If no new data - then quit. Load in the data. Load in the analysis functions. Run the analyses. Save the results. The R executable (commonly called RunProcess.R) is responsible for these tasks. We can think of it as an extremely short and extremely high-level script that implements the analysis scripts. Depending on the automated analysis, RunProcess.R can be run every two minutes (constantly checking for new data), or once a week (when we know that data will only be available on a certain day/time). 2.2.2 Bounded context Only one instance of RunProcess.R can be run at a time. Data only exists on physical folders on the system. Point #1 is important because if RunProcess.R is run every 2 minutes (constantly checking for new data) but the analyses take 3 hours to run, then we need to ensure that only one instance of RunProcess.R can be run at a time. Point #2 is important because sometimes: Data files need to be downloaded from external SFTP servers (normomo, sykdomspuls_log). Results files need to be uploaded to external SFTP servers sykdomspuls. If we include code to download/upload the files from SFTP servers inside RunProcess.R then it makes it very difficult to test RunProcess.R (because we will then need to simulate SFTP servers inside our testing infrastructure). If we know that RunProcess.R only accesses files that are available on physical folders in the system, then our testing infrastructure is a lot easier to create and maintain. 2.3 0_run.sh 2.3.1 Purpose The aim of 0_run.sh is to ensure that: The bounded context of RunProcess.R happens Run RunProcess.R Point #1 is done through the use of flock. (If neccessary) point #2 uses sshpass, sftp, and ncftpput to download/upload files from SFTP servers. 2.4 RunTest.R "],
["r-packages.html", "Chapter 3 R packages", " Chapter 3 R packages Each automated analysis has its own R package (e.g. sykdomspuls). Each R package should contain 99% "],
["intro.html", "Chapter 4 Introduction 4.1 Executive summary 4.2 What is an automated analysis? 4.3 Why not have one project for each automated analysis?", " Chapter 4 Introduction 4.1 Executive summary The dashboards project is a project at FHI concerned with running automated analyses on data. In principle, the dashboards project is split up into three parts: The overarching infrastructure (i.e. Docker containers, continuous integration, chron jobs, etc.) The R package for each automated analysis The executable for each automated analysis 4.2 What is an automated analysis? An automated analysis is any analysis that: Will be repeated multiple times in the future Always has an input dataset with consistent file structure Always has the same expected output (e.g. tables, graphs, reports) 4.3 Why not have one project for each automated analysis? Automated analyses have a lot of code and infrastructure in common. Automated analyses: Need their code to be tested via unit testing to ensure the results are correct Need their code to be tested via integration testing to ensure everything runs Need to be run at certain times Need to be able to send emails notifying people that the analyses have finished running Need to make their results accessible to the relevant people By combining them all in one umbrella project we can force everyone to use the same infrastructure, so we: Only need to solve a problem once Only need to maintain one system Can easily work on multiple projects, as we all speak the same language "]
]
